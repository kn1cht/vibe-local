# ğŸ¤–âš¡ ï¼£ï¼¬ï¼¡ï¼µï¼¤ï¼¥  ï¼¬ï¼¯ï¼£ï¼¡ï¼¬ âš¡ğŸ¤–

```
     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—   â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—
    â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â•â•
    â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—
    â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•
    â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—
     â•šâ•â•â•â•â•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â• â•šâ•â•â•â•â•â• â•šâ•â•â•â•â•â• â•šâ•â•â•â•â•â•â•
              â–ˆâ–ˆâ•—      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
              â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
              â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
              â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
              â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—
              â•šâ•â•â•â•â•â•â• â•šâ•â•â•â•â•â•  â•šâ•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•â•â•â•â•
```

> ğŸŒ´âœ¨ **Free AI Coding Environment** âœ¨ğŸŒ´
>
> No network. No cost. Local LLM agent coding.

---

## ğŸ‡¯ğŸ‡µ æ—¥æœ¬èª | [ğŸ‡ºğŸ‡¸ English](#-english) | [ğŸ‡¨ğŸ‡³ ä¸­æ–‡](#-ä¸­æ–‡)

### ã“ã‚Œã¯ä½•ï¼Ÿ

Macã«ã‚³ãƒãƒ³ãƒ‰ã‚’ã‚³ãƒ”ãƒšã™ã‚‹ã ã‘ã§AIãŒã‚³ãƒ¼ãƒ‰ã‚’æ›¸ã„ã¦ãã‚Œã‚‹ç’°å¢ƒã€‚
ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ä¸è¦ãƒ»å®Œå…¨ç„¡æ–™ã€‚Ollama + ãƒ­ãƒ¼ã‚«ãƒ«LLM ã§ Claude Code ã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’ãã®ã¾ã¾ä½¿ãˆã‚‹ã€‚

### ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« (3ã‚¹ãƒ†ãƒƒãƒ—)

**1.** ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã‚’é–‹ãï¼ˆSpotlight `Cmd+Space` â†’ "ã‚¿ãƒ¼ãƒŸãƒŠãƒ«"ã§æ¤œç´¢ï¼‰

**2.** ä»¥ä¸‹ã‚’ã‚³ãƒ”ãƒšã—ã¦Enter:

```bash
curl -fsSL https://raw.githubusercontent.com/ochyai/claude-local/main/install.sh | bash
```

**3.** æ–°ã—ã„ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã‚’é–‹ã„ã¦èµ·å‹•:

```bash
claude-local
```

### ä½¿ã„æ–¹

```bash
# å¯¾è©±ãƒ¢ãƒ¼ãƒ‰ï¼ˆAIã¨ä¼šè©±ã—ãªãŒã‚‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼‰
claude-local

# ãƒ¯ãƒ³ã‚·ãƒ§ãƒƒãƒˆï¼ˆ1å›ã ã‘è³ªå•ï¼‰
claude-local -p "Pythonã§ã˜ã‚ƒã‚“ã‘ã‚“ã‚²ãƒ¼ãƒ ä½œã£ã¦"

# ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯è‡ªå‹•åˆ¤å®šï¼ˆãƒãƒƒãƒˆãŒã‚ã‚Œã°Claude APIã€ãªã‘ã‚Œã°ãƒ­ãƒ¼ã‚«ãƒ«ï¼‰
claude-local --auto

# ãƒ¢ãƒ‡ãƒ«ã‚’æ‰‹å‹•æŒ‡å®š
claude-local --model qwen3:8b
```

### å¯¾å¿œç’°å¢ƒ

| ç’°å¢ƒ | ãƒ¡ãƒ¢ãƒª | ãƒ¢ãƒ‡ãƒ« | å‚™è€ƒ |
|------|--------|--------|------|
| Apple Silicon Mac (M1ä»¥é™) | 32GB+ | qwen3-coder:30b | ğŸ† **æ¨å¥¨** |
| Apple Silicon Mac (M1ä»¥é™) | 16GB | qwen3:8b | â­ ååˆ†å®Ÿç”¨çš„ |
| Apple Silicon Mac (M1ä»¥é™) | 8GB | qwen3:1.7b | æœ€ä½é™å‹•ä½œ |
| Intel Mac | 16GB+ | qwen3:8b | å‹•ä½œã™ã‚‹ãŒé…ã‚ |
| Linux (x86_64/arm64) | 16GB+ | qwen3:8b | NVIDIA GPUæ¨å¥¨ |

### ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

<details>
<summary>ğŸ’¡ ã‚ˆãã‚ã‚‹å•é¡Œã¨è§£æ±ºæ³•</summary>

**"ollama ãŒèµ·å‹•ã§ãã¾ã›ã‚“ã§ã—ãŸ"**
```bash
open -a Ollama        # macOS
ollama serve          # Linux
```

**"ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"**
```bash
ollama pull qwen3:8b
```

**"claude: command not found"**
```bash
npm install -g @anthropic-ai/claude-code
```

**ãƒ¢ãƒ‡ãƒ«ã‚’å¤‰æ›´ã—ãŸã„**
```bash
nano ~/.config/claude-local/config
# MODEL="qwen3:8b" ã‚’å¤‰æ›´
```

</details>

---

## ğŸ‡ºğŸ‡¸ English

### What is this?

A free AI coding environment you can set up with a single command on your Mac.
No network required. Completely free. Uses Ollama + local LLM with the Claude Code interface.

### Install (3 steps)

**1.** Open Terminal (Spotlight `Cmd+Space` â†’ search "Terminal")

**2.** Paste and hit Enter:

```bash
curl -fsSL https://raw.githubusercontent.com/ochyai/claude-local/main/install.sh | bash
```

**3.** Open a new terminal and run:

```bash
claude-local
```

### Usage

```bash
# Interactive mode (chat with AI while coding)
claude-local

# One-shot (ask once)
claude-local -p "Create a snake game in Python"

# Auto-detect network (uses Claude API if online, local if offline)
claude-local --auto

# Specify model manually
claude-local --model qwen3:8b
```

### Supported Environments

| Environment | RAM | Model | Notes |
|-------------|-----|-------|-------|
| Apple Silicon Mac (M1+) | 32GB+ | qwen3-coder:30b | ğŸ† **Recommended** |
| Apple Silicon Mac (M1+) | 16GB | qwen3:8b | â­ Very capable |
| Apple Silicon Mac (M1+) | 8GB | qwen3:1.7b | Minimum viable |
| Intel Mac | 16GB+ | qwen3:8b | Works but slower |
| Linux (x86_64/arm64) | 16GB+ | qwen3:8b | NVIDIA GPU recommended |

### Troubleshooting

<details>
<summary>ğŸ’¡ Common issues and solutions</summary>

**"ollama failed to start"**
```bash
open -a Ollama        # macOS
ollama serve          # Linux
```

**"model not found"**
```bash
ollama pull qwen3:8b
```

**"claude: command not found"**
```bash
npm install -g @anthropic-ai/claude-code
```

**Change model**
```bash
nano ~/.config/claude-local/config
# Change MODEL="qwen3:8b"
```

</details>

---

## ğŸ‡¨ğŸ‡³ ä¸­æ–‡

### è¿™æ˜¯ä»€ä¹ˆï¼Ÿ

åœ¨Macä¸Šåªéœ€å¤åˆ¶ç²˜è´´ä¸€ä¸ªå‘½ä»¤ï¼ŒAIå°±èƒ½å¸®ä½ å†™ä»£ç ã€‚
æ— éœ€ç½‘ç»œï¼Œå®Œå…¨å…è´¹ã€‚ä½¿ç”¨ Ollama + æœ¬åœ°å¤§è¯­è¨€æ¨¡å‹ï¼Œäº«å— Claude Code çš„ç•Œé¢ä½“éªŒã€‚

### å®‰è£…ï¼ˆ3æ­¥ï¼‰

**1.** æ‰“å¼€ç»ˆç«¯ï¼ˆSpotlight `Cmd+Space` â†’ æœç´¢"ç»ˆç«¯"æˆ–"Terminal"ï¼‰

**2.** ç²˜è´´ä»¥ä¸‹å‘½ä»¤å¹¶æŒ‰å›è½¦ï¼š

```bash
curl -fsSL https://raw.githubusercontent.com/ochyai/claude-local/main/install.sh | bash
```

**3.** æ‰“å¼€æ–°ç»ˆç«¯å¹¶è¿è¡Œï¼š

```bash
claude-local
```

### ä½¿ç”¨æ–¹æ³•

```bash
# äº¤äº’æ¨¡å¼ï¼ˆä¸AIå¯¹è¯ç¼–ç¨‹ï¼‰
claude-local

# å•æ¬¡æ‰§è¡Œï¼ˆåªé—®ä¸€æ¬¡ï¼‰
claude-local -p "ç”¨Pythonå†™ä¸€ä¸ªè´ªåƒè›‡æ¸¸æˆ"

# è‡ªåŠ¨æ£€æµ‹ç½‘ç»œï¼ˆæœ‰ç½‘ç”¨Claude APIï¼Œæ²¡ç½‘ç”¨æœ¬åœ°ï¼‰
claude-local --auto

# æ‰‹åŠ¨æŒ‡å®šæ¨¡å‹
claude-local --model qwen3:8b
```

### æ”¯æŒçš„ç¯å¢ƒ

| ç¯å¢ƒ | å†…å­˜ | æ¨¡å‹ | å¤‡æ³¨ |
|------|------|------|------|
| Apple Silicon Mac (M1åŠä»¥ä¸Š) | 32GB+ | qwen3-coder:30b | ğŸ† **æ¨è** |
| Apple Silicon Mac (M1åŠä»¥ä¸Š) | 16GB | qwen3:8b | â­ è¶³å¤Ÿå®ç”¨ |
| Apple Silicon Mac (M1åŠä»¥ä¸Š) | 8GB | qwen3:1.7b | æœ€ä½é™è¿è¡Œ |
| Intel Mac | 16GB+ | qwen3:8b | å¯è¿è¡Œä½†è¾ƒæ…¢ |
| Linux (x86_64/arm64) | 16GB+ | qwen3:8b | æ¨èNVIDIA GPU |

### æ•…éšœæ’é™¤

<details>
<summary>ğŸ’¡ å¸¸è§é—®é¢˜åŠè§£å†³æ–¹æ³•</summary>

**"ollama æ— æ³•å¯åŠ¨"**
```bash
open -a Ollama        # macOS
ollama serve          # Linux
```

**"æœªæ‰¾åˆ°æ¨¡å‹"**
```bash
ollama pull qwen3:8b
```

**"claude: command not found"**
```bash
npm install -g @anthropic-ai/claude-code
```

**æ›´æ¢æ¨¡å‹**
```bash
nano ~/.config/claude-local/config
# ä¿®æ”¹ MODEL="qwen3:8b"
```

</details>

---

## ğŸ”§ Architecture

```
User
  â†“
claude-local (launch script)
  â†“
Claude Code CLI (UI + agent features)
  â†“
anthropic-ollama-proxy (API translation)
  â†“
Ollama (local LLM runtime)
  â†“
qwen3-coder:30b (AI model)
```

## âš ï¸ Notes

- Local LLM accuracy is lower than Claude API
- First model download takes time (several GB to 20GB)
- Uses `--dangerously-skip-permissions` â€” for local use only
- Use `claude-local --auto` to auto-switch to Claude API when online

## ğŸ“„ License

MIT
