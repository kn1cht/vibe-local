# ğŸ¤–âš¡ ï¼¶ ï¼© ï¼¢ ï¼¥  ï¼¬ ï¼¯ ï¼£ ï¼¡ ï¼¬ âš¡ğŸ¤–

```
    â–ˆâ–ˆâ•—   â–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—
    â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â•â•
    â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—
    â•šâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•
     â•šâ–ˆâ–ˆâ–ˆâ–ˆâ•”â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—
      â•šâ•â•â•â•  â•šâ•â•â•šâ•â•â•â•â•â• â•šâ•â•â•â•â•â•â•
              â–ˆâ–ˆâ•—      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
              â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
              â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
              â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
              â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—
              â•šâ•â•â•â•â•â•â• â•šâ•â•â•â•â•â•  â•šâ•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•â•â•â•â•
```

> ğŸŒ´âœ¨ **Free AI Coding Environment** âœ¨ğŸŒ´
>
> No network. No cost. Local LLM agent coding.

**ğŸ‡¯ğŸ‡µ** ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ã®ãƒ¯ãƒ¼ã‚¯ã‚·ãƒ§ãƒƒãƒ—ã§AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ä½¿ã£ã¦å­¦ç¿’è€…ã‚’ã‚µãƒãƒ¼ãƒˆã—ãŸã‚Šã€æœ‰æ–™ãƒ—ãƒ©ãƒ³ã«æœªåŠ å…¥ã®å­¦ç”ŸãŒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’ç·´ç¿’ã—ãŸã‚Šã€ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ãªã„ç’°å¢ƒã§è‡ªç„¶è¨€èªã‚’ä½¿ã£ã¦ã‚¿ãƒ¼ãƒŸãƒŠãƒ«æ“ä½œã‚’å­¦ã‚“ã ã‚Šâ€•â€•ãã‚“ãªå ´é¢ã‚’æƒ³å®šã—ãŸã€éå–¶åˆ©ã®ç ”ç©¶ãƒ»æ•™è‚²ç›®çš„ã®ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ„ãƒ¼ãƒ«ã§ã™ã€‚

**ğŸ‡ºğŸ‡¸** Built for offline workshops where instructors support learners with AI agents, for students without paid plans who want to practice agent coding, and for beginners learning terminal operations through natural language â€” a non-profit research and education utility.

**ğŸ‡¨ğŸ‡³** é¢å‘ç¦»çº¿å·¥ä½œåŠä¸­ä½¿ç”¨AIä»£ç†è¾…åŠ©å­¦ä¹ è€…ã€æœªè®¢é˜…ä»˜è´¹è®¡åˆ’çš„å­¦ç”Ÿç»ƒä¹ ä»£ç†ç¼–ç¨‹ã€ä»¥åŠåˆå­¦è€…é€šè¿‡è‡ªç„¶è¯­è¨€å­¦ä¹ ç»ˆç«¯æ“ä½œç­‰åœºæ™¯ï¼Œè¿™æ˜¯ä¸€ä¸ªéè¥åˆ©æ€§çš„ç ”ç©¶ä¸æ•™è‚²å®ç”¨å·¥å…·ã€‚

---

## ğŸ‡¯ğŸ‡µ æ—¥æœ¬èª | [ğŸ‡ºğŸ‡¸ English](#-english) | [ğŸ‡¨ğŸ‡³ ä¸­æ–‡](#-ä¸­æ–‡)

### ã“ã‚Œã¯ä½•ï¼Ÿ

Macã«ã‚³ãƒãƒ³ãƒ‰ã‚’ã‚³ãƒ”ãƒšã™ã‚‹ã ã‘ã§AIãŒã‚³ãƒ¼ãƒ‰ã‚’æ›¸ã„ã¦ãã‚Œã‚‹ç’°å¢ƒã€‚
ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ä¸è¦ãƒ»å®Œå…¨ç„¡æ–™ã€‚Ollama + ãƒ­ãƒ¼ã‚«ãƒ«LLM ã§ Claude Code ã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’ãã®ã¾ã¾ä½¿ãˆã‚‹ã€‚

### ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« (3ã‚¹ãƒ†ãƒƒãƒ—)

**1.** ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã‚’é–‹ãï¼ˆSpotlight `Cmd+Space` â†’ "ã‚¿ãƒ¼ãƒŸãƒŠãƒ«"ã§æ¤œç´¢ï¼‰

**2.** ä»¥ä¸‹ã‚’ã‚³ãƒ”ãƒšã—ã¦Enter:

```bash
curl -fsSL https://raw.githubusercontent.com/ochyai/vibe-local/main/install.sh | bash
```

**3.** æ–°ã—ã„ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã‚’é–‹ã„ã¦èµ·å‹•:

```bash
vibe-local
```

### ä½¿ã„æ–¹

```bash
# å¯¾è©±ãƒ¢ãƒ¼ãƒ‰ï¼ˆAIã¨ä¼šè©±ã—ãªãŒã‚‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼‰
vibe-local

# ãƒ¯ãƒ³ã‚·ãƒ§ãƒƒãƒˆï¼ˆ1å›ã ã‘è³ªå•ï¼‰
vibe-local -p "Pythonã§ã˜ã‚ƒã‚“ã‘ã‚“ã‚²ãƒ¼ãƒ ä½œã£ã¦"

# ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯è‡ªå‹•åˆ¤å®šï¼ˆãƒãƒƒãƒˆãŒã‚ã‚Œã°Claude APIã€ãªã‘ã‚Œã°ãƒ­ãƒ¼ã‚«ãƒ«ï¼‰
vibe-local --auto

# ãƒ¢ãƒ‡ãƒ«ã‚’æ‰‹å‹•æŒ‡å®š
vibe-local --model qwen3:8b
```

### å¯¾å¿œç’°å¢ƒ

| ç’°å¢ƒ | ãƒ¡ãƒ¢ãƒª | ãƒ¢ãƒ‡ãƒ« | å‚™è€ƒ |
|------|--------|--------|------|
| Apple Silicon Mac (M1ä»¥é™) | 32GB+ | qwen3-coder:30b | ğŸ† **æ¨å¥¨** |
| Apple Silicon Mac (M1ä»¥é™) | 16GB | qwen3:8b | â­ ååˆ†å®Ÿç”¨çš„ |
| Apple Silicon Mac (M1ä»¥é™) | 8GB | qwen3:1.7b | æœ€ä½é™å‹•ä½œ |
| Intel Mac | 16GB+ | qwen3:8b | å‹•ä½œã™ã‚‹ãŒé…ã‚ |
| Linux (x86_64/arm64) | 16GB+ | qwen3:8b | NVIDIA GPUæ¨å¥¨ |

### ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

<details>
<summary>ğŸ’¡ ã‚ˆãã‚ã‚‹å•é¡Œã¨è§£æ±ºæ³•</summary>

**"ollama ãŒèµ·å‹•ã§ãã¾ã›ã‚“ã§ã—ãŸ"**
```bash
open -a Ollama        # macOS
ollama serve          # Linux
```

**"ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"**
```bash
ollama pull qwen3:8b
```

**"claude: command not found"**
```bash
npm install -g @anthropic-ai/claude-code
```

**ãƒ¢ãƒ‡ãƒ«ã‚’å¤‰æ›´ã—ãŸã„**
```bash
nano ~/.config/vibe-local/config
# MODEL="qwen3:8b" ã‚’å¤‰æ›´
```

</details>

---

## ğŸ‡ºğŸ‡¸ English

### What is this?

A free AI coding environment you can set up with a single command on your Mac.
No network required. Completely free. Uses Ollama + local LLM with the Claude Code interface.

### Install (3 steps)

**1.** Open Terminal (Spotlight `Cmd+Space` â†’ search "Terminal")

**2.** Paste and hit Enter:

```bash
curl -fsSL https://raw.githubusercontent.com/ochyai/vibe-local/main/install.sh | bash
```

**3.** Open a new terminal and run:

```bash
vibe-local
```

### Usage

```bash
# Interactive mode (chat with AI while coding)
vibe-local

# One-shot (ask once)
vibe-local -p "Create a snake game in Python"

# Auto-detect network (uses Claude API if online, local if offline)
vibe-local --auto

# Specify model manually
vibe-local --model qwen3:8b
```

### Supported Environments

| Environment | RAM | Model | Notes |
|-------------|-----|-------|-------|
| Apple Silicon Mac (M1+) | 32GB+ | qwen3-coder:30b | ğŸ† **Recommended** |
| Apple Silicon Mac (M1+) | 16GB | qwen3:8b | â­ Very capable |
| Apple Silicon Mac (M1+) | 8GB | qwen3:1.7b | Minimum viable |
| Intel Mac | 16GB+ | qwen3:8b | Works but slower |
| Linux (x86_64/arm64) | 16GB+ | qwen3:8b | NVIDIA GPU recommended |

### Troubleshooting

<details>
<summary>ğŸ’¡ Common issues and solutions</summary>

**"ollama failed to start"**
```bash
open -a Ollama        # macOS
ollama serve          # Linux
```

**"model not found"**
```bash
ollama pull qwen3:8b
```

**"claude: command not found"**
```bash
npm install -g @anthropic-ai/claude-code
```

**Change model**
```bash
nano ~/.config/vibe-local/config
# Change MODEL="qwen3:8b"
```

</details>

---

## ğŸ‡¨ğŸ‡³ ä¸­æ–‡

### è¿™æ˜¯ä»€ä¹ˆï¼Ÿ

åœ¨Macä¸Šåªéœ€å¤åˆ¶ç²˜è´´ä¸€ä¸ªå‘½ä»¤ï¼ŒAIå°±èƒ½å¸®ä½ å†™ä»£ç ã€‚
æ— éœ€ç½‘ç»œï¼Œå®Œå…¨å…è´¹ã€‚ä½¿ç”¨ Ollama + æœ¬åœ°å¤§è¯­è¨€æ¨¡å‹ï¼Œäº«å— Claude Code çš„ç•Œé¢ä½“éªŒã€‚

### å®‰è£…ï¼ˆ3æ­¥ï¼‰

**1.** æ‰“å¼€ç»ˆç«¯ï¼ˆSpotlight `Cmd+Space` â†’ æœç´¢"ç»ˆç«¯"æˆ–"Terminal"ï¼‰

**2.** ç²˜è´´ä»¥ä¸‹å‘½ä»¤å¹¶æŒ‰å›è½¦ï¼š

```bash
curl -fsSL https://raw.githubusercontent.com/ochyai/vibe-local/main/install.sh | bash
```

**3.** æ‰“å¼€æ–°ç»ˆç«¯å¹¶è¿è¡Œï¼š

```bash
vibe-local
```

### ä½¿ç”¨æ–¹æ³•

```bash
# äº¤äº’æ¨¡å¼ï¼ˆä¸AIå¯¹è¯ç¼–ç¨‹ï¼‰
vibe-local

# å•æ¬¡æ‰§è¡Œï¼ˆåªé—®ä¸€æ¬¡ï¼‰
vibe-local -p "ç”¨Pythonå†™ä¸€ä¸ªè´ªåƒè›‡æ¸¸æˆ"

# è‡ªåŠ¨æ£€æµ‹ç½‘ç»œï¼ˆæœ‰ç½‘ç”¨Claude APIï¼Œæ²¡ç½‘ç”¨æœ¬åœ°ï¼‰
vibe-local --auto

# æ‰‹åŠ¨æŒ‡å®šæ¨¡å‹
vibe-local --model qwen3:8b
```

### æ”¯æŒçš„ç¯å¢ƒ

| ç¯å¢ƒ | å†…å­˜ | æ¨¡å‹ | å¤‡æ³¨ |
|------|------|------|------|
| Apple Silicon Mac (M1åŠä»¥ä¸Š) | 32GB+ | qwen3-coder:30b | ğŸ† **æ¨è** |
| Apple Silicon Mac (M1åŠä»¥ä¸Š) | 16GB | qwen3:8b | â­ è¶³å¤Ÿå®ç”¨ |
| Apple Silicon Mac (M1åŠä»¥ä¸Š) | 8GB | qwen3:1.7b | æœ€ä½é™è¿è¡Œ |
| Intel Mac | 16GB+ | qwen3:8b | å¯è¿è¡Œä½†è¾ƒæ…¢ |
| Linux (x86_64/arm64) | 16GB+ | qwen3:8b | æ¨èNVIDIA GPU |

### æ•…éšœæ’é™¤

<details>
<summary>ğŸ’¡ å¸¸è§é—®é¢˜åŠè§£å†³æ–¹æ³•</summary>

**"ollama æ— æ³•å¯åŠ¨"**
```bash
open -a Ollama        # macOS
ollama serve          # Linux
```

**"æœªæ‰¾åˆ°æ¨¡å‹"**
```bash
ollama pull qwen3:8b
```

**"claude: command not found"**
```bash
npm install -g @anthropic-ai/claude-code
```

**æ›´æ¢æ¨¡å‹**
```bash
nano ~/.config/vibe-local/config
# ä¿®æ”¹ MODEL="qwen3:8b"
```

</details>

---

## ğŸ”§ Architecture

```
User
  â†“
vibe-local (launch script)
  â†“
Claude Code CLI (UI + agent features)
  â†“
anthropic-ollama-proxy (API translation)
  â†“
Ollama (local LLM runtime)
  â†“
qwen3-coder:30b (AI model)
```

## ğŸš¨ Security / ãƒªã‚¹ã‚¯ã«ã¤ã„ã¦ / å®‰å…¨é¡»çŸ¥

### ğŸ‡¯ğŸ‡µ æ—¥æœ¬èª

> **ã“ã®ãƒ„ãƒ¼ãƒ«ã¯è‡ªå·±è²¬ä»»ã§ã”åˆ©ç”¨ãã ã•ã„ã€‚**

`vibe-local` ã¯åˆå›èµ·å‹•æ™‚ã« **ãƒ„ãƒ¼ãƒ«è‡ªå‹•è¨±å¯ãƒ¢ãƒ¼ãƒ‰** (`--dangerously-skip-permissions`) ã‚’ä½¿ã†ã‹ç¢ºèªã—ã¾ã™ã€‚
è‡ªå‹•è¨±å¯ãƒ¢ãƒ¼ãƒ‰ã‚’é¸ã¶ã¨ã€AIãŒãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿æ›¸ããƒ»ã‚³ãƒãƒ³ãƒ‰å®Ÿè¡Œãƒ»ã‚·ã‚¹ãƒ†ãƒ æ“ä½œã‚’ **ç¢ºèªãªã—ã§** å®Ÿè¡Œã—ã¾ã™ã€‚

- ãƒ­ãƒ¼ã‚«ãƒ«LLMã¯ã‚¯ãƒ©ã‚¦ãƒ‰AI (Claude) ã‚ˆã‚Š **ç²¾åº¦ãŒä½ã„** ãŸã‚ã€æ„å›³ã—ãªã„æ“ä½œãŒå®Ÿè¡Œã•ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™
- é‡è¦ãªãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã§ã®ä½¿ç”¨ã¯æ…é‡ã«è¡Œã£ã¦ãã ã•ã„
- å¿ƒé…ãªå ´åˆã¯èµ·å‹•æ™‚ã« `n` ã‚’é¸æŠã™ã‚‹ã¨ã€æ¯å›ç¢ºèªã‚’æ±‚ã‚ã‚‹é€šå¸¸ãƒ¢ãƒ¼ãƒ‰ã§å‹•ãã¾ã™
- `-y` ãƒ•ãƒ©ã‚°ã§ç¢ºèªã‚’ã‚¹ã‚­ãƒƒãƒ—ã§ãã¾ã™ãŒã€ãƒªã‚¹ã‚¯ã‚’ç†è§£ã—ãŸä¸Šã§ã”åˆ©ç”¨ãã ã•ã„

```bash
vibe-local        # æ¯å›ãƒ‘ãƒ¼ãƒŸãƒƒã‚·ãƒ§ãƒ³ç¢ºèªã‚ã‚Šï¼ˆåˆå›ã«é¸æŠï¼‰
vibe-local -y     # ç¢ºèªã‚¹ã‚­ãƒƒãƒ—ï¼ˆè‡ªå‹•è¨±å¯ãƒ¢ãƒ¼ãƒ‰ï¼‰
```

### ğŸ‡ºğŸ‡¸ English

> **Use this tool at your own risk.**

On first launch, `vibe-local` asks whether to enable **auto-approve mode** (`--dangerously-skip-permissions`).
In auto-approve mode, the AI can read/write files, execute commands, and modify your system **without asking**.

- Local LLMs are **less accurate** than cloud AI (Claude), so unintended actions may occur
- Be careful when using in directories with important files
- Choose `n` at the prompt to use normal mode (asks before each tool use)
- The `-y` flag skips the prompt â€” only use it if you understand the risks

```bash
vibe-local        # Permission check on first launch
vibe-local -y     # Skip check (auto-approve mode)
```

### ğŸ‡¨ğŸ‡³ ä¸­æ–‡

> **ä½¿ç”¨æœ¬å·¥å…·é£é™©è‡ªè´Ÿã€‚**

é¦–æ¬¡å¯åŠ¨æ—¶ï¼Œ`vibe-local` ä¼šè¯¢é—®æ˜¯å¦å¯ç”¨ **å·¥å…·è‡ªåŠ¨æ‰¹å‡†æ¨¡å¼** (`--dangerously-skip-permissions`)ã€‚
åœ¨è‡ªåŠ¨æ‰¹å‡†æ¨¡å¼ä¸‹ï¼ŒAIå¯ä»¥è¯»å†™æ–‡ä»¶ã€æ‰§è¡Œå‘½ä»¤ã€ä¿®æ”¹ç³»ç»Ÿï¼Œ**æ— éœ€ç¡®è®¤**ã€‚

- æœ¬åœ°LLMçš„ç²¾åº¦ **ä½äº** äº‘ç«¯AI (Claude)ï¼Œå¯èƒ½æ‰§è¡Œéé¢„æœŸæ“ä½œ
- åœ¨åŒ…å«é‡è¦æ–‡ä»¶çš„ç›®å½•ä¸­ä½¿ç”¨æ—¶è¯·è°¨æ…
- é€‰æ‹© `n` å°†ä½¿ç”¨æ™®é€šæ¨¡å¼ï¼ˆæ¯æ¬¡å·¥å…·ä½¿ç”¨å‰è¯¢é—®ï¼‰
- `-y` å‚æ•°è·³è¿‡ç¡®è®¤ - è¯·åœ¨ç†è§£é£é™©åä½¿ç”¨

```bash
vibe-local        # é¦–æ¬¡å¯åŠ¨æ—¶ç¡®è®¤æƒé™
vibe-local -y     # è·³è¿‡ç¡®è®¤ï¼ˆè‡ªåŠ¨æ‰¹å‡†æ¨¡å¼ï¼‰
```

---

## âš™ï¸ Notes

- Local LLM accuracy is lower than Claude API
- First model download takes time (several GB to 20GB)
- Use `vibe-local --auto` to auto-switch to Claude API when online

---

## ğŸ“œ Disclaimer / å…è²¬äº‹é … / å…è´£å£°æ˜

### ğŸ‡¯ğŸ‡µ

> **æœ¬ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯ Anthropic ç¤¾ã¨ã¯ä¸€åˆ‡é–¢ä¿‚ã‚ã‚Šã¾ã›ã‚“ã€‚**
> Anthropic ãŒæä¾›ãƒ»æ¨å¥¨ãƒ»ä¿è¨¼ã™ã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚
> ã€ŒClaudeã€ã¯ Anthropic, PBC ã®å•†æ¨™ã§ã™ã€‚æœ¬ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯éå…¬å¼ã®ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ãƒ„ãƒ¼ãƒ«ã§ã™ã€‚
>
> æœ¬ãƒ„ãƒ¼ãƒ«ã¯ Claude Code CLI ã‚’éæ¨™æº–ã®æ–¹æ³•ã§ä½¿ç”¨ã—ã¦ã„ã¾ã™ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ãƒ—ãƒ­ã‚­ã‚·çµŒç”±ã§ã‚µãƒ¼ãƒ‰ãƒ‘ãƒ¼ãƒ†ã‚£LLMã«æ¥ç¶šï¼‰ã€‚
> Claude Code CLI ã®åˆ©ç”¨è¦ç´„ã«æŠµè§¦ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚åˆ©ç”¨è€…ã¯è‡ªèº«ã§åˆ©ç”¨è¦ç´„ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚
>
> æœ¬ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ã¯ç¾çŠ¶æœ‰å§¿ï¼ˆAS ISï¼‰ã§æä¾›ã•ã‚Œã€æ˜ç¤ºçš„ãƒ»æš—ç¤ºçš„ã‚’å•ã‚ãšã€ã„ã‹ãªã‚‹ä¿è¨¼ã‚‚ã‚ã‚Šã¾ã›ã‚“ã€‚
> ä½¿ç”¨ã«ã‚ˆã£ã¦ç”Ÿã˜ãŸã„ã‹ãªã‚‹æå®³ã«ã¤ã„ã¦ã‚‚ã€è‘—è€…ã¯ä¸€åˆ‡è²¬ä»»ã‚’è² ã„ã¾ã›ã‚“ã€‚
> **ã™ã¹ã¦è‡ªå·±è²¬ä»»ã§ã”åˆ©ç”¨ãã ã•ã„ã€‚**

### ğŸ‡ºğŸ‡¸

> **This project is NOT affiliated with, endorsed by, or associated with Anthropic.**
> "Claude" is a trademark of Anthropic, PBC. This is an unofficial community tool.
>
> This tool uses the Claude Code CLI in a non-standard way (connecting to third-party LLMs via a local proxy).
> This may not comply with the Claude Code CLI's terms of service. Users should review the terms themselves.
>
> Third-party dependencies (Ollama, Qwen models, Node.js, etc.) have their own licenses and terms.
>
> THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND.
> The authors are not liable for any damages arising from the use of this software.
> **Use entirely at your own risk.**

### ğŸ‡¨ğŸ‡³

> **æœ¬é¡¹ç›®ä¸ Anthropic å…¬å¸æ— ä»»ä½•å…³è”ã€‚**
> é Anthropic æä¾›ã€æ¨èæˆ–æ‹…ä¿ã€‚"Claude"æ˜¯ Anthropic, PBC çš„å•†æ ‡ã€‚æœ¬é¡¹ç›®æ˜¯éå®˜æ–¹ç¤¾åŒºå·¥å…·ã€‚
>
> æœ¬å·¥å…·ä»¥éæ ‡å‡†æ–¹å¼ä½¿ç”¨ Claude Code CLIï¼ˆé€šè¿‡æœ¬åœ°ä»£ç†è¿æ¥ç¬¬ä¸‰æ–¹LLMï¼‰ã€‚
> è¿™å¯èƒ½ä¸ç¬¦åˆ Claude Code CLI çš„æœåŠ¡æ¡æ¬¾ã€‚ç”¨æˆ·åº”è‡ªè¡Œç¡®è®¤ç›¸å…³æ¡æ¬¾ã€‚
>
> ç¬¬ä¸‰æ–¹ä¾èµ–ï¼ˆOllamaã€Qwenæ¨¡å‹ã€Node.jsç­‰ï¼‰æœ‰å„è‡ªçš„è®¸å¯è¯å’Œä½¿ç”¨æ¡æ¬¾ã€‚
>
> æœ¬è½¯ä»¶æŒ‰"åŸæ ·"æä¾›ï¼Œä¸æä¾›ä»»ä½•æ˜ç¤ºæˆ–æš—ç¤ºçš„ä¿è¯ã€‚
> ä½œè€…ä¸å¯¹å› ä½¿ç”¨æœ¬è½¯ä»¶è€Œäº§ç”Ÿçš„ä»»ä½•æŸå®³æ‰¿æ‹…è´£ä»»ã€‚
> **ä½¿ç”¨æœ¬å·¥å…·é£é™©å®Œå…¨è‡ªè´Ÿã€‚**

## ğŸ“„ License

MIT
